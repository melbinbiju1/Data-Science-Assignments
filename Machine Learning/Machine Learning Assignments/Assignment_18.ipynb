{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is the difference between supervised and unsupervised learning? Give some examples to illustrate your point.**\n",
    "\n",
    "**Solution:-** The main distinction between the two approaches is the use of labeled datasets. To put it simply, supervised learning uses labeled input and output data, while an unsupervised learning algorithm does not. Unsupervised learning models, in contrast, work on their own to discover the inherent structure of unlabeled data.\n",
    "\n",
    "In Supervised learning, you train the machine using data which is well “labeled.” Unsupervised learning is a machine learning technique, where you do not need to supervise the model. For example, Baby can identify other dogs based on past supervised learning.\n",
    "\n",
    "![ima](https://miro.medium.com/max/1400/0*Uzqy-gqZg77Wun0e.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Mention a few unsupervised learning applications.**\n",
    "\n",
    "**Solution:-** \n",
    "The main applications of unsupervised learning include clustering, visualization, dimensionality reduction, finding association rules, and anomaly detection.\n",
    "\n",
    "These are some of the commonly used clustering algorithms:\n",
    "\n",
    "- K-Means\n",
    "- Expectation Maximization\n",
    "- Hierarchical Cluster Analysis (HCA)\n",
    "\n",
    "These are some of the most common dimensionality reduction algorithms in machine learning:\n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "- Kernel PCA\n",
    "- Locally-Linear Embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What are the three main types of clustering methods? Briefly describe the characteristics of each.**\n",
    "\n",
    "**Solution:-** The various types of clustering method is as follows:\n",
    "- **Connectivity-based Clustering (Hierarchical clustering):** Hierarchical Clustering is a method of unsupervised machine learning clustering where it begins with a pre-defined top to bottom hierarchy of clusters. It then proceeds to perform a decomposition of the data objects based on this hierarchy, hence obtaining the clusters. This method  follows two approaches based on the direction of progress, i.e., whether it is the top-down or bottom-up flow of creating clusters.\n",
    "\n",
    "![1](https://www.analytixlabs.co.in/blog/wp-content/uploads/2020/07/image-3-28-1-600x400.jpg)\n",
    "\n",
    "![2](https://www.analytixlabs.co.in/blog/wp-content/uploads/2020/07/image-4-17-1-600x323.jpg)\n",
    "\n",
    "- **Centroids-based Clustering (Partitioning methods):** Centroid based clustering is considered as one of the most simplest clustering algorithms, yet the most effective way of creating clusters and assigning data points to it. The intuition behind centroid based clustering is that a cluster is characterized and represented by a central vector and data points that are in close proximity to these vectors are assigned to the respective clusters.  \n",
    "![3](https://www.analytixlabs.co.in/blog/wp-content/uploads/2020/07/image-5-15-1-600x323.jpg) \n",
    "\n",
    "- **Density-based Clustering (Model-based methods):** If one looks into the previous two methods that we discussed, one would observe that both hierarchical and centroid based algorithms are dependent on a distance (similarity/proximity)  metric. The very definition of a cluster is based on this metric. Density-based clustering methods take density into consideration  instead of distances. Clusters are considered as the densest region in a data space, which is separated by regions of lower object density and it is defined as a maximal-set of connected points.\n",
    "       \n",
    "![4](https://www.analytixlabs.co.in/blog/wp-content/uploads/2020/07/image-6-11-1-600x323.jpg)\n",
    "\n",
    "- **Distribution-based Clustering:** Until now, the clustering techniques as we know are based around either proximity (similarity/distance) or composition (density). There is a family of clustering algorithms that take a totally different metric into consideration – probability. Distribution-based clustering creates and groups data points   based on their likely hood of belonging to the same probability distribution (Gaussian, Binomial etc.) in the data.\n",
    "![5](https://www.analytixlabs.co.in/blog/wp-content/uploads/2020/07/image-7-9-1-600x323.jpg) \n",
    "\n",
    "**Distribution-based Clustering:**\n",
    "\n",
    "- **Fuzzy Clustering:** The general idea about clustering revolves around assigning data points to mutually exclusive  clusters, meaning, a data point always resides uniquely inside a cluster and it cannot belong to more than one cluster. Fuzzy clustering methods change this paradigm by assigning a data-point to multiple clusters with a quantified degree of belongingness metric. The data-points that are in proximity to the center of a cluster, may  also belong in the cluster that is at a higher degree than points in the edge of a cluster. The possibility of which  an element belongs to a given cluster is measured by membership coefficient that vary from 0 to 1.\n",
    "\n",
    "- **Constraint Clustering: (Supervised Clustering)**\n",
    "![6](https://www.analytixlabs.co.in/blog/wp-content/uploads/2020/07/image-8-6-1-600x400.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Explain how the k-means algorithm determines the consistency of clustering.**\n",
    "\n",
    "**Solution:-** \n",
    "Calculate the Within-Cluster-Sum of Squared Errors (WSS) for different values of k, and choose the k for which WSS becomes first starts to diminish. In the plot of WSS-versus-k, this is visible as an elbow. \n",
    "Within-Cluster-Sum of Squared Errors sounds a bit complex however it is not.\n",
    "\n",
    "1. K centroids are created randomly (based on the predefined value of K) \n",
    "\n",
    "2. K-means allocates every data point in the dataset to the nearest centroid (minimizing Euclidean distances between them), meaning that a data point is considered to be in a particular cluster if it is closer to that cluster’s centroid than any other centroid. \n",
    "\n",
    "3. Then K-means recalculates the centroids by taking the mean of all data points assigned to that centroid’s cluster, hence reducing the total intra-cluster variance in relation to the previous step. The “means” in the K-means refers to averaging the data and finding the new centroid. \n",
    "\n",
    "4. The algorithm iterates between steps 2 and 3 until some criteria is met (e.g. the sum of distances between the data points and their corresponding centroid is minimized, a maximum number of iterations is reached, no changes in centroids value or no data points change clusters)\n",
    "\n",
    "![1](https://th.bing.com/th/id/OIP.riInbzp5CiuMOOq8rldQ7wHaDc?pid=ImgDet&rs=1)\n",
    "\n",
    "![2](https://www.researchgate.net/profile/Vinicius-Rofatto/publication/341622556/figure/fig5/AS:895083989704708@1590416061547/Within-Cluster-Sum-of-Squares-WCSS-for-Para-State.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. With a simple illustration, explain the key difference between the k-means and k-medoids algorithms.**\n",
    "\n",
    "**Solution:-** K-means tries to minimize the total squared error, whereas k-medoids minimizes the sum of dissimilarities between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k -means algorithm, k -medoids chooses datapoints as centers (medoids or exemplars)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. What is a dendrogram, and how does it work? Explain how to do it.**\n",
    "\n",
    "\n",
    "**Solution:-** It is a diagram that helps to do the visualization for hierarchical clustering. The main objective of using the Dendrogram is to make an optimal number of clusters and to allocate objects to clusters.\n",
    "\n",
    "Consider the closest point and make it a cluster-based upon Euclidean Distance. y-axis is the closest Euclidean Distance and the x-axis is having all data points. Again, find the clusters that are having the least distance between them.\n",
    "\n",
    "Now, p2 and p3 are having its own cluster and p1 is closest to it so we create a full cluster and plot it on the dendrogram graph.\n",
    "\n",
    "Similarly, with P6, P5, and P4\n",
    "\n",
    "Now, combine 2 clusters into a single cluster, and our dendrogram graph is ready.\n",
    "\n",
    "So from the above example, we have created a dendrogram no we need to find what is an optimal number of the cluster from the dendrogram. Let us draw a horizontal line on the dendrogram graph.\n",
    "\n",
    "It means the number of lines cut from the threshold horizontal line that is the number of clusters formed. So 2 clusters are formed from the above example.\n",
    "\n",
    "So, now the important question is when the distance is the largest, we can set that line as the threshold.\n",
    "\n",
    "So, the line cutting the largest distance is a threshold line that is, 2 clusters are formed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. What exactly is SSE? What role does it play in the k-means algorithm?**\n",
    "\n",
    "**Solution:-** SSE means Sum of Squared Errors.\n",
    "The regression line is the line made using the function we defined above. To get the SSE we calculate the distance for each of the data points from the regression line then square the it, then we add to the sum.\n",
    "\n",
    "The SSE is defined as the sum of the squared Euclidean distances of each point to its closest centroid. Since this is a measure of error, the objective of k-means is to try to minimize this value. The purpose of this is to show that the initialization of the centroids is an important step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. With a step-by-step algorithm, explain the k-means procedure.**\n",
    "\n",
    "**Solution:-**\n",
    "\n",
    "k-means is one of the simplest unsupervised learning algorithms that solve the well known clustering problem. The procedure follows a simple and easy to classify a given dataset through a certain number of clusters (assume k clusters) fixed apriori. The main idea is to define k centers, one for each cluster.\n",
    "\n",
    "- Step 1: Choose the number of clusters k. \n",
    "- Step 2: Select k random points from the data as centroids.\n",
    "- Step 3: Assign all the points to the closest cluster centroid. \n",
    "- Step 4: Recompute the centroids of newly formed clusters.\n",
    "- Step 5: Repeat steps 3 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. In the sense of hierarchical clustering, define the terms single link and complete link.**\n",
    "\n",
    "**Solution:-** In single-link clustering or single-linkage clustering , the similarity of two clusters is the similarity of their most similar members . This single-link merge criterion is local. We pay attention solely to the area where the two clusters come closest to each other. Other, more distant parts of the cluster and the clusters' overall structure are not taken into account.\n",
    "\n",
    "In complete-link clustering or complete-linkage clustering , the similarity of two clusters is the similarity of their most dissimilar members. This is equivalent to choosing the cluster pair whose merge has the smallest diameter. This complete-link merge criterion is non-local; the entire structure of the clustering can influence merge decisions. This results in a preference for compact clusters with small diameters over long, straggly clusters, but also causes sensitivity to outliers. A single document far from the center can increase diameters of candidate merge clusters dramatically and completely change the final clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. How does the apriori concept aid in the reduction of measurement overhead in a business basket analysis? Give an example to demonstrate your point.**\n",
    "\n",
    "**Solution:-** The Apriori algorithm uses frequent itemsets to generate association rules, and it is designed to work on the databases that contain transactions. With the help of these association rule, it determines how strongly or how weakly two objects are connected. This algorithm uses a breadth-first search and Hash Tree to calculate the itemset associations efficiently. It is the iterative process for finding the frequent itemsets from the large dataset. It is mainly used for market basket analysis and helps to find those products that can be bought together. It can also be used in the healthcare field to find drug reactions for patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
