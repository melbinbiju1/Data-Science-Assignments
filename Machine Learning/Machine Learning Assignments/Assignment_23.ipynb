{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What are the key reasons for reducing the dimensionality of a dataset? What are the major disadvantages?**\n",
    "\n",
    "**Solution:-** In machine learning classification problems, there are often too many factors on the basis of which the final classification is done. These factors are independent features. The higher the number of features, the difficult to visualize the training set and then work on them. Sometimes, most of these features are correlated (multicollinear), and hence redundant. This is where dimensionality reduction algorithms come into play. \n",
    "\n",
    "**Dimensionality reduction** is the process of reducing the number of random variables under consideration, by obtaining a set of principal variables. It is divided into feature selection and feature extraction. \n",
    "\n",
    "**Disadvantages of Dimensionality Reduction:**\n",
    "\n",
    "- It may lead to some amount of data loss.\n",
    "- PCA tends to find linear correlations between variables, which is sometimes undesirable.\n",
    "- PCA fails in cases where mean and covariance are not enough to define datasets.\n",
    "Further, we may not know how many principal components to keep- in practice, some thumb rules are applied.\n",
    "\n",
    "**2. What is the dimensionality curse?**\n",
    "\n",
    "**Solution:-** The curse of dimensionality means **error increases with the increase in the number of features.** It refers to the fact that algorithms are harder to design in high dimensions and often have a running time exponential in the dimensions and increases computational time. \n",
    "\n",
    "**3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how can you go about doing it? If not, what is the reason?**\n",
    "\n",
    "**Solution:-** Once a dataset's dimensionality has been reduced using one of the algorithms as discussed earlier, it is almost always impossible to perfectly reverse the operation, because some information is lost during dimensionality reduction. \n",
    "\n",
    "However, some algorithms (such as PCA) have a simple reverse transformation procedure that can reconstruct a dataset relatively similar to the original, other algorithms (such as T-SNE) do not Dimensionality reduction (compression of information) is reversible in auto-encoders. \n",
    "\n",
    "Auto-encoder is regular neural network with bottleneck layer in the middle. You have for instance 20 inputs in the first layer, 10 neurons in the middle layer and again 20 neurons in the last layer. When you train such network you force it to compress information to 10 neurons and then uncompress again minimizing error in the last layer (desired output vector equals input vector). When you use well known Back-propagation algorithm to train such network it performs PCA - Principal Component Analysis. **PCA returns uncorrelated features. By using more sophisticated algorithm to train auto-encoder you can make it perform nonlinear ICA - Independent Component Analysis**. ICA returns statistically independent features. This training algorithm searches for low complexity neural networks with high generalization capability. As a byproduct of regularization you get ICA.\n",
    "\n",
    "**4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?**\n",
    "\n",
    "**Solution:-** PCA can at least get rid of useless dimensions, which will significantly reduce the demensionality of most datasets. \n",
    "If dataset is comprised of points that are perfectly aligned, **PCA can reduce the dataset down to 1 dimension and preserve 95% of the variance.**\n",
    "\n",
    "However, reducing demensionality of datasets and reducing the uselful dimensions/ data will loose too much info. \"You want to unroll the swiss roll, not squash it\", you **can still do a PCA computation on nonlinear data - but the results will be meaningless, beyond decomposing to the dominant linear modes and provided a global linear representation of the spread of the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?**\n",
    "\n",
    "**Solution:-** \n",
    "1. Vanilla: works well when the dataset fits in memory\n",
    "\n",
    "2. Incremental: useful for large datasets that don't fit in memory, but is slower than regular and Used for online tasks.\n",
    "\n",
    "3. Randomized: useful for considerably reduce dimensionality and the dataset fits in memory\n",
    "\n",
    "4. Kernel: Useful for nonlinear datasets\n",
    "\n",
    "![1](https://i.stack.imgur.com/Q7HIP.gif)\n",
    "\n",
    "\n",
    "**7. How do you assess a dimensionality reduction algorithm's success on your dataset?**\n",
    "\n",
    "**Solution:-** Intuitively, a dimensionality reduction algorithm performs well if it eliminates a lot of dimensions from the dataset without losing too much information. One way to measure this is to apply the reverse transformation and measure the reconstruction error. \n",
    "\n",
    "Alternatively, if you are using dimensionality reduction as a preprocessing step before another Machine Learning algorithm (e.g., a Random Forest classifier), then you can simply measure the performance of that second algorithm; if dimensionality reduction did not lose too much information, then the algorithm should perform just as well as when using the original dataset.\n",
    "\n",
    "**8. Is it logical to use two different dimensionality reduction algorithms in a chain?**\n",
    "\n",
    "**Solution:-** You can chain two different algorithms, such as PCA to quickly get rid of unnecessary data and then LLE (Locall Linear Embedded) that works slower. This will likely yield the same performance, but with faster results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
